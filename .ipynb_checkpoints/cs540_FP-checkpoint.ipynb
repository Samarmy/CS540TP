{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import io\n",
    "\n",
    "# you'll need to run nltk.download() and download all packages\n",
    "#nltk.download_shell()\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.data.path.append('D:\\\\JacobSchool\\\\CS540\\\\nltk_data')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets from http://www.iaees.org/publications/journals/selforganizology/articles/2016-3(3)/algorithm-to-transform-natural-language-into-SQL-queries.pdf\n",
    "Escape Word Set<br/>\n",
    "Expression Mapping Set<br/>\n",
    "Noun Set<br/>\n",
    "Verb Set<br/>\n",
    "Semantic Set<br/>\n",
    "Variable Set<br/>\n",
    "Relation Set<br/>\n",
    "Attribute Set<br/>\n",
    "Conjunction Set<br/>\n",
    "This is all the sets for words and rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "escape_words = set(stopwords.words('english'))#[\"a\", \"an\", \"the\", \"which\", \"is\", \"of\", \"with\", \"to\", \"for\", \"are\", \"and\", \"should\", \"be\"]\n",
    "\n",
    "rules_on_top_of = []\n",
    "\n",
    "rules_side_by_side = []\n",
    "\n",
    "nouns = [\"block\", \"wildcard\", \"block0\", \"block1\", \"block2\", \"block3\", \"block4\", \"block5\", \"block6\", \"block7\", \"block8\", \"block9\" \\\n",
    "         \"block10\", \"block11\", \"block12\", \"block13\", \"block14\", \"block15\", \"block16\", \"block17\", \"block18\", \"block19\" \\\n",
    "         \"block20\", \"block21\", \"block22\", \"block23\", \"block24\", \"block25\", \"block26\", \"block27\", \"block28\", \"block29\" \\\n",
    "         \"block30\", \"block31\", \"block32\", \"block33\", \"block34\", \"block35\", \"block36\", \"block37\", \"block38\", \"block39\" \\\n",
    "         \"block40\", \"block41\", \"block42\", \"block43\", \"block44\", \"block45\", \"block46\", \"block47\", \"block48\", \"block49\" \\\n",
    "         \"block50\", \"block51\", \"block52\", \"block53\", \"block54\", \"block55\", \"block56\", \"block57\", \"block58\", \"block59\" \\\n",
    "         \"block60\", \"block61\", \"block62\", \"block63\", \"block64\", \"block65\", \"block66\", \"block67\", \"block68\", \"block69\" \\\n",
    "         \"block70\", \"block71\", \"block72\", \"block73\", \"block74\", \"block75\", \"block76\", \"block77\", \"block78\", \"block79\" \\\n",
    "         \"block80\", \"block81\", \"block82\", \"block83\", \"block84\", \"block85\", \"block86\", \"block87\", \"block88\", \"block89\" \\\n",
    "         \"block90\", \"block91\", \"block92\", \"block93\", \"block94\", \"block95\", \"block96\", \"block97\", \"block98\", \"block99\" \\\n",
    "         \"wildcard0\", \"wildcard1\", \"wildcard2\", \"wildcard3\", \"wildcard4\", \"wildcard5\", \"wildcard6\", \"wildcard7\", \"wildcard8\", \"wildcard9\" \\\n",
    "         \"wildcard10\", \"wildcard11\", \"wildcard12\", \"wildcard13\", \"wildcard14\", \"wildcard15\", \"wildcard16\", \"wildcard17\", \"wildcard18\", \"wildcard19\" \\\n",
    "         \"wildcard20\", \"wildcard21\", \"wildcard22\", \"wildcard23\", \"wildcard24\", \"wildcard25\", \"wildcard26\", \"wildcard27\", \"wildcard28\", \"wildcard29\" \\\n",
    "         \"wildcard30\", \"wildcard31\", \"wildcard32\", \"wildcard33\", \"wildcard34\", \"wildcard35\", \"wildcard36\", \"wildcard37\", \"wildcard38\", \"wildcard39\" \\\n",
    "         \"wildcard40\", \"wildcard41\", \"wildcard42\", \"wildcard43\", \"wildcard44\", \"wildcard45\", \"wildcard46\", \"wildcard47\", \"wildcard48\", \"wildcard49\" \\\n",
    "         \"wildcard50\", \"wildcard51\", \"wildcard52\", \"wildcard53\", \"wildcard54\", \"wildcard55\", \"wildcard56\", \"wildcard57\", \"wildcard58\", \"wildcard59\" \\\n",
    "         \"wildcard60\", \"wildcard61\", \"wildcard62\", \"wildcard63\", \"wildcard64\", \"wildcard65\", \"wildcard66\", \"wildcard67\", \"wildcard68\", \"wildcard69\" \\\n",
    "         \"wildcard70\", \"wildcard71\", \"wildcard72\", \"wildcard73\", \"wildcard74\", \"wildcard75\", \"wildcard76\", \"wildcard77\", \"wildcard78\", \"wildcard79\" \\\n",
    "         \"wildcard80\", \"wildcard81\", \"wildcard82\", \"wildcard83\", \"wildcard84\", \"wildcard85\", \"wildcard86\", \"wildcard87\", \"wildcard88\", \"wildcard89\" \\\n",
    "         \"wildcard90\", \"wildcard91\", \"wildcard92\", \"wildcard93\", \"wildcard94\", \"wildcard95\", \"wildcard96\", \"wildcard97\", \"wildcard98\", \"wildcard99\" ]\n",
    "\n",
    "verbs_top = [\"top\", \"above\", \"over\", \"on\"]\n",
    "verbs_below = [\"below\", \"underneath\",  \"under\"]\n",
    "verbs_side = [\"side\", \"neighboring\", \"beside\", \"next\"]\n",
    "\n",
    "variables_colors = [\"amber\", \"amethyst\", \"apricot\", \"aquamarine\", \"azure\", \"beige\", \"black\", \"blue\", \"blush\", \"bronze\", \"brown\", \"burgundy\", \"byzantium\", \"carmine\", \"cerise\", \"cerulean\", \"champagne\", \"chocolate\", \"coffee\", \"copper\", \"coral\", \"crimson\", \"cyan\", \"emerald\", \"erin\", \"gold\", \"gray\", \"green\", \"harlequin\", \"indigo\", \"ivory\", \"jade\", \"lavender\", \"lemon\", \"lilac\", \"lime\", \"magenta\", \"maroon\", \"mauve\", \"navy\", \"ochre\", \"olive\", \"orange\", \"orchid\", \"peach\", \"pearl\", \"periwinkle\", \"pink\", \"plum\", \"puce\", \"purple\", \"raspberry\", \"red\", \"rose\", \"ruby\", \"salmon\", \"sangria\", \"sapphire\", \"scarlet\", \"silver\", \"tan\", \"taupe\", \"teal\", \"turquoise\", \"ultramarine\", \"violet\", \"viridian\", \"white\", \"yellow\"]\n",
    "variables_numbers = [\"0\",\"zero\",\"1\", \"one\", \"2\", \"two\", \"3\", \"three\", \"4\", \"four\", \"5\", \"five\", \"6\", \"six\", \"7\", \"seven\", \"8\", \"eight\", \"9\", \"nine\"]\n",
    "loc_options = [\"x\", \"y\", \"z\",\"location\", \"located\", \"spot\", \"position\", \"address\", \"place\", \"locality\", \"point\", \"placement\", \"locale\", \"setting\", \"bearings\", \"bearing\", \"venue\"]\n",
    "\n",
    "#position, place, situation, site, locality, locale, spot, whereabouts, point, placement; scene, setting, area, environment; bearings, orientation; venue, address;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Tags:<br/>\n",
    "NN &rightarrow; Noun<br/>\n",
    "VT &rightarrow; Verb Top<br/>\n",
    "VB &rightarrow; Verb Below<br/>\n",
    "VS &rightarrow; Verb Side<br/>\n",
    "VarC &rightarrow; Variable Color<br/>\n",
    "VarL &rightarrow; Variable Location<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLocation(tagged_sentence):\n",
    "        m_grammar = \"\"\"LOCATION:  {<INT><INT><INT>}\"\"\"\n",
    "        chunkParser = nltk.RegexpParser(m_grammar)\n",
    "        tree = chunkParser.parse(tagged_sentence)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == \"LOCATION\":\n",
    "                x=subtree.leaves()[0][0]\n",
    "                y=subtree.leaves()[1][0]\n",
    "                z=subtree.leaves()[2][0]\n",
    "                #print(\"LOCATION: \"+str(subtree.leaves()[]))\n",
    "                \n",
    "        m_grammar = \"\"\"LOCATION:  {<VarL><INT>}\"\"\"\n",
    "        chunkParser = nltk.RegexpParser(m_grammar)\n",
    "        tree = chunkParser.parse(tagged_sentence)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == \"LOCATION\":\n",
    "                if(subtree.leaves()[0][0]==\"x\"):\n",
    "                    x=subtree.leaves()[1][0]\n",
    "                if(subtree.leaves()[0][0]==\"y\"):\n",
    "                    y=subtree.leaves()[1][0]\n",
    "                if(subtree.leaves()[0][0]==\"z\"):\n",
    "                    z=subtree.leaves()[1][0]                \n",
    "        return str(str(x)+ \" \"+str(y)+\" \"+str(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def checkBlocks(words):\n",
    "        parts_of_speech = []\n",
    "        parts_speach = nltk.pos_tag(words)\n",
    "        parts_of_speech.append(parts_speach)\n",
    "        \n",
    "        p=enumerate_parts(parts_of_speech)\n",
    "        m_grammar = \"\"\"BLOCK:  {<BNN><INT>}\"\"\"\n",
    "        chunkParser = nltk.RegexpParser(m_grammar)\n",
    "        tree = chunkParser.parse(p)\n",
    "        blockChanged=False\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == \"BLOCK\":\n",
    "                indexToReplace=parts_speach.index(subtree.leaves()[0])\n",
    "                parts_speach.pop(indexToReplace+1)\n",
    "                block=str(subtree.leaves()[0][0]+subtree.leaves()[1][0])\n",
    "                parts_speach[indexToReplace]=(block,\"BNN\")\n",
    "                blockChanged=True\n",
    "                \n",
    "        if(blockChanged):\n",
    "            temp=[]\n",
    "            for i in parts_speach:\n",
    "                temp.append(i[0])\n",
    "            words=temp\n",
    "        return words\n",
    "        \n",
    "        \n",
    "def enumerate_parts(parts_of_speech):\n",
    "    \n",
    "    for p in parts_of_speech:\n",
    "        for i,w in enumerate(p):\n",
    "            if(w[0] in nouns):\n",
    "                tw = list(w)\n",
    "                tw[1]='BNN'\n",
    "                w=tuple(tw)\n",
    "                p[i]=w\n",
    "            elif(w[0] in variables_colors):\n",
    "                tw = list(w)\n",
    "                tw[1]='VARC'\n",
    "                w=tuple(tw)\n",
    "                p[i]=w\n",
    "            elif(w[0] in verbs_top):\n",
    "                tw = list(w)\n",
    "                tw[1]='VT'\n",
    "                w=tuple(tw)\n",
    "                p[i]=w\n",
    "            elif(w[0] in verbs_below):\n",
    "                tw = list(w)\n",
    "                tw[1]='VB'\n",
    "                w=tuple(tw)\n",
    "                p[i]=w\n",
    "            elif(w[0] in verbs_side):\n",
    "                tw = list(w)\n",
    "                tw[1]='VS'\n",
    "                w=tuple(tw)\n",
    "                p[i]=w\n",
    "            elif(w[0] in loc_options):\n",
    "                tw = list(w)\n",
    "                tw[1]='VARL'\n",
    "                w=tuple(tw)\n",
    "                p[i]=w\n",
    "            elif(w[0] in variables_numbers):\n",
    "                tw = list(w)\n",
    "                tw[1]='INT'\n",
    "                w=tuple(tw)\n",
    "                p[i]=w\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def createBlockWorld(tagged_sentence, outputfile):\n",
    "    block1=\"\"\n",
    "    block2=\"\"\n",
    "    relation=\"\"\n",
    "    command=\"\"\n",
    "    location=\"\"\n",
    "    color=\"\"\n",
    "    swap_on_top_of = False\n",
    "\n",
    "    try:\n",
    "        #Find the first block, should be the block\n",
    "        location_sentence=False\n",
    "        for ts in tagged_sentence:\n",
    "            if (ts[1] == \"VT\"):\n",
    "                relation = \"on-top-of\"\n",
    "\n",
    "            if (ts[1] == \"VS\"):\n",
    "                relation = \"side-by-side\"\n",
    "\n",
    "            if (ts[1] == \"VB\"):\n",
    "                swap_on_top_of = True\n",
    "                relation = \"on-top-of\"\n",
    "\n",
    "            if (ts[1] == \"VarL\"):\n",
    "                location_sentence=True\n",
    "                break\n",
    "\n",
    "\n",
    "            if ((\"block\" in str(ts[0])) or (\"wildcard\" in str(ts[0]))) and block1 == \"\":\n",
    "                block1=ts[0]\n",
    "\n",
    "            if (ts[0] != block1) and ((\"block\" in str(ts[0])) or (\"wildcard\" in str(ts[0]))):\n",
    "                    block2=ts[0]\n",
    "            if (ts[1] == \"VarC\"):\n",
    "                color = ts[0]\n",
    "\n",
    "        if(location_sentence):\n",
    "            location=processLocation(tagged_sentence)            \n",
    "\n",
    "        thecommand = \"\"\n",
    "        #Create the command for an is relation\n",
    "        if block1 and block2 and relation:\n",
    "            if swap_on_top_of:\n",
    "                thecommand = \"(is %s %s %s)\" %(block2, block1, relation)\n",
    "            else:\n",
    "                thecommand = \"(is %s %s %s)\" %(block1, block2, relation)\n",
    "\n",
    "        if block1 and color:\n",
    "            thecommand = \"(has %s color %s)\" % (block1,color)\n",
    "\n",
    "        if block1 and location and (len(location)==5):\n",
    "                thecommand = \"(has %s location %s)\" % (block1,location)\n",
    "\n",
    "\n",
    "        if thecommand != \"\":\n",
    "            print(thecommand)\n",
    "            with open(outputfile, \"a\") as myfile:\n",
    "                myfile.write(thecommand+\"\\n\")\n",
    "        return \"SUCCESS\"\n",
    "    except:\n",
    "        return \"FAILURE\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partOfSpeechTagging(words):\n",
    "        #Part of Speech tagging\n",
    "        parts_of_speech = []\n",
    "\n",
    "        parts_speach = nltk.pos_tag(words)\n",
    "        parts_of_speech.append(parts_speach)\n",
    "         \n",
    "        \n",
    "        p=enumerate_parts(parts_of_speech)\n",
    "                \n",
    "        m_grammar = \"\"\"ONTOP: {<VT>.*<VT>.*<IN>.*<BNN>}\n",
    "                        LOCATION: {<VARL>.*<CD>*<CD>*<CD>}\n",
    "                                {<VARL>.*<VBZ>.*<CD>*<CD>*<CD>}\n",
    "                        BLOCK:  {<BNN>.<CD>}\n",
    "                                {<BNN>?}\n",
    "                        COLOR: {<VARC.*>}\"\"\"\n",
    "\n",
    "\n",
    "        #grammar = ('''Location: {<NN>*<CD>*<CD>*<CD>} # NP''')\n",
    "        chunkParser = nltk.RegexpParser(m_grammar)\n",
    "        tree = chunkParser.parse(p)\n",
    "        for subtree in tree.subtrees():\n",
    "            \n",
    "            if subtree.label() == \"LOCATION\":\n",
    "                print(\"LOCATION: \"+str(subtree.leaves()))\n",
    "            if subtree.label() == \"BLOCK\":\n",
    "                print(\"BLOCK: \"+str(subtree.leaves()))\n",
    "            if subtree.label() == \"COLOR\":\n",
    "                print(\"COLOR: \"+str(subtree.leaves()))\n",
    "            if subtree.label() == \"ONTOP\":\n",
    "                print(\"ONTOP: \"+str(subtree.leaves()))\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(has block1 location 5 4 3)\n",
      "BLOCK: [('block1', 'BNN')]\n",
      "\n",
      "(has block2 location 4 7 9)\n",
      "BLOCK: [('block2', 'BNN')]\n",
      "\n",
      "(has block3 location 0 9 4)\n",
      "BLOCK: [('block3', 'BNN')]\n",
      "\n",
      "(has block4 location 3 2 1)\n",
      "BLOCK: [('block4', 'BNN')]\n",
      "\n",
      "(has block5 location 5 9 1)\n",
      "BLOCK: [('block5', 'BNN')]\n",
      "\n",
      "(has block6 location 4 4 3)\n",
      "BLOCK: [('block6', 'BNN')]\n",
      "\n",
      "BLOCK: [('block7', 'BNN')]\n",
      "\n",
      "BLOCK: [('block8', 'BNN')]\n",
      "\n",
      "(is block block8 side-by-side)\n",
      "BLOCK: [('block', 'BNN')]\n",
      "BLOCK: [('block8', 'BNN')]\n",
      "\n",
      "BLOCK: [('block11', 'BNN')]\n",
      "\n",
      "(is block block14 on-top-of)\n",
      "BLOCK: [('block', 'BNN')]\n",
      "BLOCK: [('block', 'BNN')]\n",
      "BLOCK: [('block13', 'BNN')]\n",
      "ONTOP: [('on', 'VT'), ('top', 'VT'), ('of', 'IN'), ('block14', 'BNN')]\n",
      "\n",
      "BLOCK: [('block13', 'BNN')]\n",
      "\n",
      "(has block location 8 9 0)\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "BLOCK: [('block', 'BNN')]\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "BLOCK: [('block', 'BNN')]\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "(has block location 7 8 9)\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "(has block location 9 4 2)\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "(has block location 9 4 2)\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "(has block location 9 3 0)\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "BLOCK: [('block', 'BNN')]\n",
      "ONTOP: [('on', 'VT'), ('top', 'VT'), ('of', 'IN'), ('block', 'BNN')]\n",
      "\n",
      "BLOCK: [('block', 'BNN')]\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "BLOCK: [('block', 'BNN')]\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "BLOCK: [('block', 'BNN')]\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "BLOCK: [('block', 'BNN')]\n",
      "BLOCK: [('block', 'BNN')]\n",
      "\n",
      "18: SUCCESS\n",
      "8: SUCCESS\n",
      "7: SUCCESS\n",
      "14: SUCCESS\n",
      "17: SUCCESS\n",
      "9: SUCCESS\n",
      "23: SUCCESS\n",
      "16: SUCCESS\n",
      "20: SUCCESS\n",
      "12: SUCCESS\n",
      "49: SUCCESS\n",
      "13: SUCCESS\n",
      "21: SUCCESS\n",
      "6: SUCCESS\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-238-1e5d7d0f4b10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0moutputStart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"aStarSearch/nlp_outputs/start3.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mreadinfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nlp_tests/start3.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputStart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;31m#Read in goal file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#outputGoal = \"aStarSearch/nlp_outputs/goal1.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "def readinfile(inputfile, outputfile):\n",
    "    #os.remove(inputfile) if os.path.exists(inputfile) else None\n",
    "    os.remove(outputfile) if os.path.exists(outputfile) else None\n",
    "\n",
    "    commandFile = open(inputfile) \n",
    "    data = commandFile.read()# Use this to read file content as a stream: \n",
    "    commandFile.close()\n",
    "\n",
    "    sentences = sent_tokenize(data)\n",
    "    #print(sentences)\n",
    "\n",
    "    sentence_complexity={}\n",
    "    cfdist = ConditionalFreqDist()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        temp = []\n",
    "        for w in words:\n",
    "            temp.append(w.lower())\n",
    "\n",
    "        temp=checkBlocks(temp)\n",
    "        \n",
    "        words = temp[0:-1]\n",
    "        temp = []\n",
    "\n",
    "        for w in words:\n",
    "            if(w in nouns):\n",
    "                temp.append((w, \"NN\"))\n",
    "            elif(w in verbs_top):\n",
    "                temp.append((w, \"VT\"))\n",
    "            elif(w in verbs_below):\n",
    "                temp.append((w, \"VB\"))\n",
    "            elif(w in verbs_side):\n",
    "                temp.append((w, \"VS\"))\n",
    "            elif(w in variables_colors):\n",
    "                temp.append((w, \"VarC\"))\n",
    "            elif(w in loc_options):\n",
    "                temp.append((w, \"VarL\"))\n",
    "            elif(w in variables_numbers):\n",
    "                temp.append((w, \"INT\"))                \n",
    "     \n",
    "        tagged_words = temp\n",
    "        \n",
    "        if(createBlockWorld(tagged_words, outputfile)==\"SUCCESS\"):\n",
    "             condition = len(words)\n",
    "             cfdist[condition][\"SUCCESS\"] += 1\n",
    "             #print(condition)\n",
    "        else:\n",
    "             condition = len(words)\n",
    "             cfdist[condition][\"Fail\"] += 1\n",
    "             #print(condition)\n",
    "    \n",
    "        partOfSpeechTagging(words)\n",
    "    \n",
    "    for f in cfdist:\n",
    "        for k in cfdist[f]:\n",
    "            print(str(f)+\": \" +str(k))\n",
    "            #print(\"Frequency of\", f, cfdist.freq(f))\n",
    "\n",
    "#Read in initial file\n",
    "outputStart = \"aStarSearch/nlp_outputs/start3.txt\"\n",
    "readinfile(\"nlp_tests/start3.txt\", outputStart)\n",
    "\n",
    "#Matt testing \"start3 file for graphing purposes.  please use start1 for normal testing\"\n",
    "sys.exit()\n",
    "#Read in goal file\n",
    "outputGoal = \"aStarSearch/nlp_outputs/goal1.txt\"\n",
    "readinfile(\"nlp_tests/goal1.txt\", outputGoal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install pandas\n",
    "#import pandas as pd\n",
    "%run -i \"./aStarSearch/aStarSearch.py\" $outputStart $outputGoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
