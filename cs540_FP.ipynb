{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import io\n",
    "import speech_recognition as sr\n",
    "\n",
    "# you'll need to run nltk.download() and download all packages\n",
    "#nltk.download_shell()\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.data.path.append('D:\\\\JacobSchool\\\\CS540\\\\nltk_data')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets from http://www.iaees.org/publications/journals/selforganizology/articles/2016-3(3)/algorithm-to-transform-natural-language-into-SQL-queries.pdf\n",
    "Escape Word Set<br/>\n",
    "Expression Mapping Set<br/>\n",
    "Noun Set<br/>\n",
    "Verb Set<br/>\n",
    "Semantic Set<br/>\n",
    "Variable Set<br/>\n",
    "Relation Set<br/>\n",
    "Attribute Set<br/>\n",
    "Conjunction Set<br/>\n",
    "This is all the sets for words and rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "escape_words = set(stopwords.words('english'))#[\"a\", \"an\", \"the\", \"which\", \"is\", \"of\", \"with\", \"to\", \"for\", \"are\", \"and\", \"should\", \"be\"]\n",
    "\n",
    "rules_on_top_of = []\n",
    "\n",
    "rules_side_by_side = []\n",
    "\n",
    "nouns = [\"block\", \"wildcard\", \"block0\", \"block1\", \"block2\", \"block3\", \"block4\", \"block5\", \"block6\", \"block7\", \"block8\", \"block9\" \\\n",
    "         \"block10\", \"block11\", \"block12\", \"block13\", \"block14\", \"block15\", \"block16\", \"block17\", \"block18\", \"block19\" \\\n",
    "         \"block20\", \"block21\", \"block22\", \"block23\", \"block24\", \"block25\", \"block26\", \"block27\", \"block28\", \"block29\" \\\n",
    "         \"block30\", \"block31\", \"block32\", \"block33\", \"block34\", \"block35\", \"block36\", \"block37\", \"block38\", \"block39\" \\\n",
    "         \"block40\", \"block41\", \"block42\", \"block43\", \"block44\", \"block45\", \"block46\", \"block47\", \"block48\", \"block49\" \\\n",
    "         \"block50\", \"block51\", \"block52\", \"block53\", \"block54\", \"block55\", \"block56\", \"block57\", \"block58\", \"block59\" \\\n",
    "         \"block60\", \"block61\", \"block62\", \"block63\", \"block64\", \"block65\", \"block66\", \"block67\", \"block68\", \"block69\" \\\n",
    "         \"block70\", \"block71\", \"block72\", \"block73\", \"block74\", \"block75\", \"block76\", \"block77\", \"block78\", \"block79\" \\\n",
    "         \"block80\", \"block81\", \"block82\", \"block83\", \"block84\", \"block85\", \"block86\", \"block87\", \"block88\", \"block89\" \\\n",
    "         \"block90\", \"block91\", \"block92\", \"block93\", \"block94\", \"block95\", \"block96\", \"block97\", \"block98\", \"block99\" \\\n",
    "         \"wildcard0\", \"wildcard1\", \"wildcard2\", \"wildcard3\", \"wildcard4\", \"wildcard5\", \"wildcard6\", \"wildcard7\", \"wildcard8\", \"wildcard9\" \\\n",
    "         \"wildcard10\", \"wildcard11\", \"wildcard12\", \"wildcard13\", \"wildcard14\", \"wildcard15\", \"wildcard16\", \"wildcard17\", \"wildcard18\", \"wildcard19\" \\\n",
    "         \"wildcard20\", \"wildcard21\", \"wildcard22\", \"wildcard23\", \"wildcard24\", \"wildcard25\", \"wildcard26\", \"wildcard27\", \"wildcard28\", \"wildcard29\" \\\n",
    "         \"wildcard30\", \"wildcard31\", \"wildcard32\", \"wildcard33\", \"wildcard34\", \"wildcard35\", \"wildcard36\", \"wildcard37\", \"wildcard38\", \"wildcard39\" \\\n",
    "         \"wildcard40\", \"wildcard41\", \"wildcard42\", \"wildcard43\", \"wildcard44\", \"wildcard45\", \"wildcard46\", \"wildcard47\", \"wildcard48\", \"wildcard49\" \\\n",
    "         \"wildcard50\", \"wildcard51\", \"wildcard52\", \"wildcard53\", \"wildcard54\", \"wildcard55\", \"wildcard56\", \"wildcard57\", \"wildcard58\", \"wildcard59\" \\\n",
    "         \"wildcard60\", \"wildcard61\", \"wildcard62\", \"wildcard63\", \"wildcard64\", \"wildcard65\", \"wildcard66\", \"wildcard67\", \"wildcard68\", \"wildcard69\" \\\n",
    "         \"wildcard70\", \"wildcard71\", \"wildcard72\", \"wildcard73\", \"wildcard74\", \"wildcard75\", \"wildcard76\", \"wildcard77\", \"wildcard78\", \"wildcard79\" \\\n",
    "         \"wildcard80\", \"wildcard81\", \"wildcard82\", \"wildcard83\", \"wildcard84\", \"wildcard85\", \"wildcard86\", \"wildcard87\", \"wildcard88\", \"wildcard89\" \\\n",
    "         \"wildcard90\", \"wildcard91\", \"wildcard92\", \"wildcard93\", \"wildcard94\", \"wildcard95\", \"wildcard96\", \"wildcard97\", \"wildcard98\", \"wildcard99\" ]\n",
    "\n",
    "verbs_top = [\"top\", \"above\", \"over\", \"on\"]\n",
    "verbs_below = [\"below\", \"underneath\",  \"under\"]\n",
    "verbs_side = [\"side\", \"neighboring\", \"beside\", \"next\"]\n",
    "\n",
    "variables_colors = [\"amber\", \"amethyst\", \"apricot\", \"aquamarine\", \"azure\", \"beige\", \"black\", \"blue\", \"blush\", \"bronze\", \"brown\", \"burgundy\", \"byzantium\", \"carmine\", \"cerise\", \"cerulean\", \"champagne\", \"chocolate\", \"coffee\", \"copper\", \"coral\", \"crimson\", \"cyan\", \"emerald\", \"erin\", \"gold\", \"gray\", \"green\", \"harlequin\", \"indigo\", \"ivory\", \"jade\", \"lavender\", \"lemon\", \"lilac\", \"lime\", \"magenta\", \"maroon\", \"mauve\", \"navy\", \"ochre\", \"olive\", \"orange\", \"orchid\", \"peach\", \"pearl\", \"periwinkle\", \"pink\", \"plum\", \"puce\", \"purple\", \"raspberry\", \"red\", \"rose\", \"ruby\", \"salmon\", \"sangria\", \"sapphire\", \"scarlet\", \"silver\", \"tan\", \"taupe\", \"teal\", \"turquoise\", \"ultramarine\", \"violet\", \"viridian\", \"white\", \"yellow\"]\n",
    "#variables_location = [\"0\",\"zero\",\"1\", \"one\", \"2\", \"two\", \"3\", \"three\", \"4\", \"four\", \"5\", \"five\", \"6\", \"six\", \"7\", \"seven\", \"8\", \"eight\", \"9\", \"nine\"]\n",
    "loc_options = [\"move\", \"location\", \"spot\", \"position\", \"address\", \"place\", \"locality\", \"point\", \"placement\", \"locale\", \"setting\", \"bearings\", \"bearing\", \"venue\", \"is\", \"at\"]\n",
    "\n",
    "#position, place, situation, site, locality, locale, spot, whereabouts, point, placement; scene, setting, area, environment; bearings, orientation; venue, address;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Tags:<br/>\n",
    "NN &rightarrow; Noun<br/>\n",
    "VT &rightarrow; Verb Top<br/>\n",
    "VB &rightarrow; Verb Below<br/>\n",
    "VS &rightarrow; Verb Side<br/>\n",
    "VarC &rightarrow; Variable Color<br/>\n",
    "VarL &rightarrow; Variable Location<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_parts(parts_of_speech):\n",
    "    \n",
    "    #for p in parts_of_speech:\n",
    "    for i,w in enumerate(parts_of_speech):\n",
    "        if(w[0] in nouns):\n",
    "            tw = list(w)\n",
    "            tw[1]='BNN'\n",
    "            w=tuple(tw)\n",
    "            parts_of_speech[i]=w\n",
    "        elif(w[0] in variables_colors):\n",
    "            tw = list(w)\n",
    "            tw[1]='VARC'\n",
    "            w=tuple(tw)\n",
    "            parts_of_speech[i]=w\n",
    "        elif(w[0] in verbs_top):\n",
    "            tw = list(w)\n",
    "            tw[1]='VT'\n",
    "            w=tuple(tw)\n",
    "            parts_of_speech[i]=w\n",
    "        elif(w[0] in verbs_below):\n",
    "            tw = list(w)\n",
    "            tw[1]='VBOT' #Changed to a value that is not in the default parse list\n",
    "            w=tuple(tw)\n",
    "            parts_of_speech[i]=w\n",
    "        elif(w[0] in verbs_side):\n",
    "            tw = list(w)\n",
    "            tw[1]='VS'\n",
    "            w=tuple(tw)\n",
    "            parts_of_speech[i]=w\n",
    "        elif(w[0] in loc_options):\n",
    "            tw = list(w)\n",
    "            tw[1]='VARL'\n",
    "            w=tuple(tw)\n",
    "            parts_of_speech[i]=w\n",
    "        #Not needed the CD tag represents a number\n",
    "        #elif(w[0] in variables_numbers):\n",
    "        #    tw = list(w)\n",
    "        #    tw[1]='INT'\n",
    "        #    w=tuple(tw)\n",
    "        #    p[i]=w\n",
    "    #return parts_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLocation(tagged_sentence):\n",
    "        m_grammar = \"\"\"LOCATION: {<VARL>.*<.*>.*<CD><CD><CD>}\n",
    "                                {<CD><CD><CD>}\"\"\"\n",
    "        chunkParser = nltk.RegexpParser(m_grammar)\n",
    "        tree = chunkParser.parse(tagged_sentence)\n",
    "        location = []\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == \"LOCATION\":\n",
    "                loc_values = subtree.leaves()[-3:]\n",
    "                location.append(loc_values[0][0])\n",
    "                location.append(loc_values[1][0])\n",
    "                location.append(loc_values[2][0])\n",
    "        \n",
    "        #print(\"LOCATION: \"+ str(location))\n",
    "        \n",
    "        return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBlocks(parts_of_speech):\n",
    "        m_grammar = \"\"\"BLOCK:  {<BNN>.*<CD>}\n",
    "                                {<BNN>?}\"\"\"\n",
    "        chunkParser = nltk.RegexpParser(m_grammar)\n",
    "        tree = chunkParser.parse(parts_of_speech)\n",
    "\n",
    "        blocks=[]\n",
    "        \n",
    "        #Collect the two blocks out of the parts of speech\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == \"BLOCK\":\n",
    "                #print(\"BLOCK: \"+str(subtree.leaves()))                \n",
    "                block_1 = subtree.leaves()[0][0]\n",
    "                if(len(subtree.leaves())>1):\n",
    "                    block_1 += subtree.leaves()[1][0]\n",
    "                    #print(\"Block_1: \" + block_1)\n",
    "                blocks.append(block_1)\n",
    "        #print(\"The Blocks: \" + str(blocks))\n",
    "        return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setRelation(parts_of_speech):\n",
    "    m_grammar = \"\"\"ONTOP: {<VT>.*}\n",
    "                        BOTTOM: {<VBOT>.*}\n",
    "                        SIDE: {<VS>.*}\"\"\"\n",
    "    \n",
    "    #Need to see if there is an and between the relations\n",
    "    \n",
    "    chunkParser = nltk.RegexpParser(m_grammar)\n",
    "    tree = chunkParser.parse(parts_of_speech)\n",
    "    \n",
    "    #swap_on_top_of = False\n",
    "    relations=[]\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == \"ONTOP\" or subtree.label() == \"BOTTOM\":\n",
    "            if subtree.label() == \"BOTTOM\":\n",
    "                #swap_on_top_of = True\n",
    "                relations.append(\"bottom-of\")\n",
    "            else:\n",
    "                relations.append(\"on-top-of\")\n",
    "            #print(\"ONTOP: \"+str(subtree.leaves()))\n",
    "        if subtree.label() == \"SIDE\":\n",
    "            relations.append(\"side-by-side\")\n",
    "            #print(\"SIDE: \"+str(subtree.leaves()))\n",
    "    \n",
    "    #print(\"The Relations: \" + str(relations))\n",
    "    return relations#,swap_on_top_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processColor(parts_of_speech):\n",
    "    color=\"\"\n",
    "    m_grammar = \"\"\"COLOR: {<VARC.*>}\"\"\"\n",
    "    \n",
    "    chunkParser = nltk.RegexpParser(m_grammar)\n",
    "    tree = chunkParser.parse(parts_of_speech)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == \"COLOR\":\n",
    "                #print(\"COLOR: \"+str(subtree.leaves()))\n",
    "                color = subtree.leaves()[0][0]\n",
    "                \n",
    "    #print(\"The Color: \" + color)\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkParts(p):\n",
    "    m_grammar = \"\"\"ONTOP: {<VT>.*}\n",
    "                    BOTTOM: {<VBL>.*}\n",
    "                    LOCATION: {<VARL>.*<.*>.*<CD>.*<CD>.*<CD>}\n",
    "                            {<CD>.*<CD>.*<CD>}\n",
    "                    SIDE: {<VS>.*}\n",
    "                    BLOCK:  {<BNN>.*<CD>}\n",
    "                            {<BNN>?}\n",
    "                    COLOR: {<VARC.*>}\"\"\"\n",
    "    #print(p)\n",
    "    chunkParser = nltk.RegexpParser(m_grammar)\n",
    "    return chunkParser.parse(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAndInSentence(parts_of_speech):\n",
    "    m_grammar = \"\"\"AND: {<CC>}\"\"\"\n",
    "    \n",
    "    chunkParser = nltk.RegexpParser(m_grammar)\n",
    "    tree = chunkParser.parse(parts_of_speech)\n",
    "    hasAnd = False\n",
    "    \n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == \"AND\":\n",
    "                #print(\"AND: \"+ str(subtree.leaves()))\n",
    "                hasAnd = True\n",
    "\n",
    "    return hasAnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_command(outputfile, blocks, relations, color, location, hasAnd):\n",
    "    #print(\"format_command\")\n",
    "    #print(blocks)\n",
    "    #print(relations)\n",
    "    #print(color)\n",
    "    #print(location)\n",
    "    #print(swap_on_top_of)\n",
    "    try:\n",
    "        thecommand = \"\"\n",
    "        block1=\"\"\n",
    "        block2=\"\"\n",
    "        relation=\"\"\n",
    "        relation2=\"\"\n",
    "\n",
    "        #Set the first block\n",
    "        if len(blocks) > 0:\n",
    "            block1=blocks[0]\n",
    "        \n",
    "        #Set the first relation\n",
    "        if len(relations) > 0:\n",
    "            relation=relations[0]\n",
    "            \n",
    "        #Multiple blocks and one relations\n",
    "        if (len(blocks)>1) and (len(blocks)<4) and len(relations)>0:\n",
    "            for i, block in enumerate(blocks):\n",
    "                if i > 0:\n",
    "                    block2=block\n",
    "\n",
    "                if block1 and block2 and relation:\n",
    "                    if relations[0] == \"bottom-of\":#\n",
    "                        thecommand += \"(is %s %s %s)\\n\" %(block2, block1, \"on-top-of\")\n",
    "                    else:\n",
    "                        thecommand += \"(is %s %s %s)\\n\" %(block1, block2, relation)            \n",
    "        \n",
    "        #Multiple blocks and multiple relations\n",
    "        #Assumptions is that each pair of blocks goes with one relation: 2 to 1\n",
    "        if len(blocks)>3 and len(relations)>1:\n",
    "            #Grap 2 blocks at a time\n",
    "            rel_count = 0\n",
    "            for i, block in enumerate(blocks):\n",
    "                mod = i % 2\n",
    "                if mod == 0:\n",
    "                    block1=block\n",
    "                else:\n",
    "                    block2=block\n",
    "                    if relations[rel_count] == \"bottom-of\":# swap_on_top_of:\n",
    "                        thecommand += \"(is %s %s %s)\\n\" %(block2, block1, \"on-top-of\")\n",
    "                    else:\n",
    "                        thecommand += \"(is %s %s %s)\\n\" %(block1, block2, relations[rel_count])\n",
    "\n",
    "                    if rel_count < len(relations):\n",
    "                        rel_count+=1\n",
    "\n",
    "        #print(\"Block1: \" + block1 + \" Block2: \" + block2)\n",
    "\n",
    "        #Create the command for an is relation\n",
    "        #if block1 and block2 and relation:\n",
    "        #    if relations[rel_count] == \"bottom-of\":#swap_on_top_of:\n",
    "        #        thecommand = \"(is %s %s %s)\\n\" %(block2, block1, \"on-top-of\")\n",
    "        #    else:\n",
    "        #        thecommand = \"(is %s %s %s)\\n\" %(block1, block2, relation)\n",
    "\n",
    "        if block1 and color:\n",
    "            thecommand = \"(has %s color %s)\\n\" % (block1,color)\n",
    "\n",
    "        if block1 and location and (len(location)==3):\n",
    "            thecommand = \"(has %s location %s %s %s)\\n\" % (block1,location[0],location[1],location[2])\n",
    "\n",
    "        #pdb.set_trace()\n",
    "        if thecommand != \"\":\n",
    "            print(\"Storing Command(s): \" + thecommand)\n",
    "            with open(outputfile, \"a\") as myfile:\n",
    "                myfile.write(thecommand)\n",
    "            return \"SUCCESS\"\n",
    "    except:\n",
    "        return \"FAILURE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partsOfSpeechTag(words, outputfile):\n",
    "    \n",
    "    #Part of Speech tagging    \n",
    "    parts_speech = nltk.pos_tag(words)\n",
    "    #print(parts_speech)\n",
    "    #parts_of_speech.append(parts_speach)\n",
    "        \n",
    "    #print(\"\\nParts of Speech for each sentence\")\n",
    "    #for p in parts_speech:\n",
    "    #print(\"\\nThe Parts of Speech:\")\n",
    "    print(\"Tokenized Words: \" + str(words))\n",
    "    enumerate_parts(parts_speech)\n",
    "    print(\"Parts of Speech: \" + str(parts_speech))\n",
    "    tree = chunkParts(parts_speech)\n",
    "    \n",
    "    location = []\n",
    "    block_1 = \"\"\n",
    "    block_2 = \"\"\n",
    "    color = \"\"\n",
    "    relation = \"\"\n",
    "    \n",
    "    hasAnd = processAndInSentence(parts_speech)\n",
    "    \n",
    "    #Check for location value\n",
    "    location = processLocation(parts_speech)\n",
    "    \n",
    "    #Array of blocks, allow for something like: Block 1 is next to block2 and block3.\n",
    "    #Need to generate more than one command for that scenario\n",
    "    #Will always assume the first block is the key block: Block 1 is next to block 2 and block3 is next to block4 will not work.\n",
    "    #Maybe add something to catch the \"and\", if there are 4 blocks it can generate 2 commands.\n",
    "    blocks = checkBlocks(parts_speech)\n",
    "    \n",
    "    #Set the type of relation\n",
    "    relation = setRelation(parts_speech)\n",
    "    \n",
    "    #Set the color\n",
    "    color = processColor(parts_speech)\n",
    "    \n",
    "    return format_command(outputfile, blocks, relation, color, location, hasAnd) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readinfile(inputfile, outputfile):\n",
    "    #os.remove(inputfile) if os.path.exists(inputfile) else None\n",
    "    os.remove(outputfile) if os.path.exists(outputfile) else None\n",
    "\n",
    "    commandFile = open(inputfile) \n",
    "    data = commandFile.read()# Use this to read file content as a stream: \n",
    "    commandFile.close()\n",
    "    \n",
    "    cfdist = ConditionalFreqDist()\n",
    "    \n",
    "    sentences = sent_tokenize(data)\n",
    "    #print(sentences)\n",
    "    #parts_of_speech = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        temp = []\n",
    "        for w in words:\n",
    "            temp.append(w.lower())\n",
    "        words = temp[0:-1]\n",
    "        \n",
    "        #Parts of Speech tagging\n",
    "        if(partsOfSpeechTag(words, outputfile)==\"SUCCESS\"):\n",
    "        \n",
    "        #if(format_command(outputfile, block_1, block_2, relation, color, location, swap_on_top_of)==\"SUCCESS\"):\n",
    "             condition = len(words)\n",
    "             cfdist[condition][\"SUCCESS\"] += 1\n",
    "             #print(condition)\n",
    "        else:\n",
    "             condition = len(words)\n",
    "             cfdist[condition][\"Fail\"] += 1\n",
    "             #print(condition)\n",
    "        print()\n",
    "    \n",
    "    for f in cfdist:\n",
    "        for k in cfdist[f]:\n",
    "            print(str(f)+\": \" +str(k))\n",
    "            #print(\"Frequency of\", f, cfdist.freq(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Speech Reecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()\n",
    "recording = sr.AudioFile('Recording.wav')\n",
    "with recording as source:\n",
    "    audio = r.record(source)\n",
    "recording_str = r.recognize_google(audio)\n",
    "new_file=open(\"nlp_tests/Recording.txt\",mode=\"w\",encoding=\"utf-8\")\n",
    "new_file.write(recording_str)\n",
    "new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: ['block', '1', 'has', 'color', 'red']\n",
      "Parts of Speech: [('block', 'BNN'), ('1', 'CD'), ('has', 'VBZ'), ('color', 'NN'), ('red', 'VARC')]\n",
      "Storing Command(s): (has block1 color red)\n",
      "\n",
      "\n",
      "Tokenized Words: ['block2', 'has', 'location', '8', '3', '1']\n",
      "Parts of Speech: [('block2', 'BNN'), ('has', 'VBZ'), ('location', 'VARL'), ('8', 'CD'), ('3', 'CD'), ('1', 'CD')]\n",
      "Storing Command(s): (has block2 location 8 3 1)\n",
      "\n",
      "\n",
      "Tokenized Words: ['block3', 'has', 'color', 'green']\n",
      "Parts of Speech: [('block3', 'BNN'), ('has', 'VBZ'), ('color', 'NN'), ('green', 'VARC')]\n",
      "Storing Command(s): (has block3 color green)\n",
      "\n",
      "\n",
      "Tokenized Words: ['block1', 'is', 'at', 'the', 'spot', '8', '3', '0']\n",
      "Parts of Speech: [('block1', 'BNN'), ('is', 'VARL'), ('at', 'VARL'), ('the', 'DT'), ('spot', 'VARL'), ('8', 'CD'), ('3', 'CD'), ('0', 'CD')]\n",
      "Storing Command(s): (has block1 location 8 3 0)\n",
      "\n",
      "\n",
      "Tokenized Words: ['block3', 'location', 'is', '8', '3', '2']\n",
      "Parts of Speech: [('block3', 'BNN'), ('location', 'VARL'), ('is', 'VARL'), ('8', 'CD'), ('3', 'CD'), ('2', 'CD')]\n",
      "Storing Command(s): (has block3 location 8 3 2)\n",
      "\n",
      "\n",
      "Tokenized Words: ['block', '23', 'location', 'is', '8', '4', '0']\n",
      "Parts of Speech: [('block', 'BNN'), ('23', 'CD'), ('location', 'VARL'), ('is', 'VARL'), ('8', 'CD'), ('4', 'CD'), ('0', 'CD')]\n",
      "Storing Command(s): (has block23 location 8 4 0)\n",
      "\n",
      "\n",
      "Tokenized Words: ['block', '21', 'location', 'is', '8', '5', '0']\n",
      "Parts of Speech: [('block', 'BNN'), ('21', 'CD'), ('location', 'VARL'), ('is', 'VARL'), ('8', 'CD'), ('5', 'CD'), ('0', 'CD')]\n",
      "Storing Command(s): (has block21 location 8 5 0)\n",
      "\n",
      "\n",
      "5: SUCCESS\n",
      "6: SUCCESS\n",
      "4: SUCCESS\n",
      "8: SUCCESS\n",
      "7: SUCCESS\n",
      "Tokenized Words: ['block', '1', 'is', 'next', 'to', 'block', '2', 'and', 'block3']\n",
      "Parts of Speech: [('block', 'BNN'), ('1', 'CD'), ('is', 'VARL'), ('next', 'VS'), ('to', 'TO'), ('block', 'BNN'), ('2', 'CD'), ('and', 'CC'), ('block3', 'BNN')]\n",
      "Storing Command(s): (is block1 block2 side-by-side)\n",
      "(is block1 block3 side-by-side)\n",
      "\n",
      "\n",
      "Tokenized Words: ['wildcard', '1', 'has', 'location', '4', '2', '0']\n",
      "Parts of Speech: [('wildcard', 'BNN'), ('1', 'CD'), ('has', 'VBZ'), ('location', 'VARL'), ('4', 'CD'), ('2', 'CD'), ('0', 'CD')]\n",
      "Storing Command(s): (has wildcard1 location 4 2 0)\n",
      "\n",
      "\n",
      "Tokenized Words: ['wildcard2', 'is', 'next', 'to', 'wildcard1']\n",
      "Parts of Speech: [('wildcard2', 'BNN'), ('is', 'VARL'), ('next', 'VS'), ('to', 'TO'), ('wildcard1', 'BNN')]\n",
      "Storing Command(s): (is wildcard2 wildcard1 side-by-side)\n",
      "\n",
      "\n",
      "Tokenized Words: ['block', '23', 'is', 'next', 'to', 'block', '21']\n",
      "Parts of Speech: [('block', 'BNN'), ('23', 'CD'), ('is', 'VARL'), ('next', 'VS'), ('to', 'TO'), ('block', 'BNN'), ('21', 'CD')]\n",
      "Storing Command(s): (is block23 block21 side-by-side)\n",
      "\n",
      "\n",
      "Tokenized Words: ['put', 'block', '21', 'at', '3', '5', '0']\n",
      "Parts of Speech: [('put', 'NN'), ('block', 'BNN'), ('21', 'CD'), ('at', 'VARL'), ('3', 'CD'), ('5', 'CD'), ('0', 'CD')]\n",
      "Storing Command(s): (has block21 location 3 5 0)\n",
      "\n",
      "\n",
      "9: SUCCESS\n",
      "7: SUCCESS\n",
      "5: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "#Read in initial file\n",
    "outputStart = \"aStarSearch/nlp_outputs/start1.txt\"\n",
    "readinfile(\"nlp_tests/start1.txt\", outputStart)\n",
    "\n",
    "#Read in goal file\n",
    "outputGoal = \"aStarSearch/nlp_outputs/goal1.txt\"\n",
    "readinfile(\"nlp_tests/goal1.txt\", outputGoal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aStarSearch/nlp_outputs/start1.txt\n",
      "aStarSearch/nlp_outputs/goal1.txt\n",
      "\n",
      "Searching...\n",
      "\n",
      "END STATE:\n",
      "        Coordinates Properties\n",
      "Block                         \n",
      "block1    (4, 1, 0)      [red]\n",
      "block2    (4, 0, 0)         []\n",
      "block21   (3, 5, 0)         []\n",
      "block23   (3, 4, 0)         []\n",
      "block3    (4, 2, 0)    [green]\n",
      "['command slide block21 -1 -1', 'command slide block21 -1 1', 'command slide block21 -1 1', 'command slide block21 -1 -1', 'command slide block21 -1 0', 'command slide block1 -1 -1', 'command slide block1 -1 -1', 'command slide block1 0 -1', 'command slide block23 -1 1', 'command slide block23 -1 1', 'command slide block23 -1 -1', 'command slide block23 -1 0', 'command slide block1 -1 0', 'command slide block23 -1 -1', 'command grab block3', 'command carry block3 0 1 -1', 'command carry block3 0 0 -1', 'command release block3', 'command grab block2', 'command carry block2 -1 0 -1', 'command release block2', 'command grab block2', 'command release block2', 'command slide block1 -1 1', 'command slide block3 -1 1']\n",
      "LENGTH: 25\n",
      "\n",
      "END STATE:\n",
      "        Coordinates Properties\n",
      "Block                         \n",
      "block1    (4, 1, 0)      [red]\n",
      "block2    (4, 0, 0)         []\n",
      "block21   (3, 5, 0)         []\n",
      "block23   (4, 5, 0)         []\n",
      "block3    (4, 2, 0)    [green]\n",
      "['command slide block21 -1 -1', 'command slide block21 -1 -1', 'command slide block21 -1 1', 'command slide block21 -1 1', 'command slide block21 -1 0', 'command slide block1 0 -1', 'command slide block1 -1 -1', 'command slide block1 -1 -1', 'command slide block23 -1 1', 'command slide block23 -1 1', 'command slide block23 -1 -1', 'command slide block23 -1 0', 'command slide block1 -1 0', 'command grab block3', 'command carry block3 0 0 1', 'command carry block3 0 0 -1', 'command carry block3 0 1 -1', 'command carry block3 0 0 -1', 'command release block3', 'command grab block2', 'command carry block2 -1 0 -1', 'command release block2', 'command slide block1 -1 1', 'command slide block3 -1 1']\n",
      "LENGTH: 24\n",
      "\n",
      "END STATE:\n",
      "        Coordinates Properties\n",
      "Block                         \n",
      "block1    (4, 1, 0)      [red]\n",
      "block2    (4, 0, 0)         []\n",
      "block21   (3, 5, 0)         []\n",
      "block23   (3, 4, 0)         []\n",
      "block3    (4, 2, 0)    [green]\n",
      "['command slide block21 -1 1', 'command slide block21 -1 0', 'command slide block21 -1 0', 'command slide block21 -1 0', 'command slide block21 -1 -1', 'command slide block1 0 -1', 'command slide block1 -1 -1', 'command slide block1 -1 -1', 'command slide block23 -1 1', 'command slide block23 -1 1', 'command slide block23 -1 0', 'command slide block23 -1 -1', 'command slide block23 -1 -1', 'command grab block3', 'command carry block3 -1 1 -1', 'command carry block3 0 0 -1', 'command release block3', 'command slide block1 -1 0', 'command grab block2', 'command carry block2 -1 0 -1', 'command release block2', 'command slide block1 -1 1', 'command slide block3 -1 1']\n",
      "LENGTH: 23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install pandas\n",
    "#import pandas as pd\n",
    "%run -i \"./aStarSearch/aStarSearch.py\" $outputStart $outputGoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
