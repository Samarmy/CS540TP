{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import io\n",
    "# nltk.download()\n",
    "# you'll need to run nltk.download() and download all packages\n",
    "# nltk.download_shell()\n",
    "# nltk.data.path.append('C:\\\\Users\\\\Sam\\\\AppData\\\\Roaming\\\\nltk_data')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets from http://www.iaees.org/publications/journals/selforganizology/articles/2016-3(3)/algorithm-to-transform-natural-language-into-SQL-queries.pdf\n",
    "Escape Word Set<br/>\n",
    "Expression Mapping Set<br/>\n",
    "Noun Set<br/>\n",
    "Verb Set<br/>\n",
    "Semantic Set<br/>\n",
    "Variable Set<br/>\n",
    "Relation Set<br/>\n",
    "Attribute Set<br/>\n",
    "Conjunction Set<br/>\n",
    "This is all the sets for words and rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "escape_words = set(stopwords.words('english'))#[\"a\", \"an\", \"the\", \"which\", \"is\", \"of\", \"with\", \"to\", \"for\", \"are\", \"and\", \"should\", \"be\"]\n",
    "\n",
    "rules_on_top_of = []\n",
    "\n",
    "rules_side_by_side = []\n",
    "\n",
    "nouns = [\"block0\", \"block1\", \"block2\", \"block3\", \"block4\", \"block5\", \"block6\", \"block7\", \"block8\", \"block9\" \\\n",
    "         \"block10\", \"block11\", \"block12\", \"block13\", \"block14\", \"block15\", \"block16\", \"block17\", \"block18\", \"block19\" \\\n",
    "         \"block20\", \"block21\", \"block22\", \"block23\", \"block24\", \"block25\", \"block26\", \"block27\", \"block28\", \"block29\" \\\n",
    "         \"block30\", \"block31\", \"block32\", \"block33\", \"block34\", \"block35\", \"block36\", \"block37\", \"block38\", \"block39\" \\\n",
    "         \"block40\", \"block41\", \"block42\", \"block43\", \"block44\", \"block45\", \"block46\", \"block47\", \"block48\", \"block49\" \\\n",
    "         \"block50\", \"block51\", \"block52\", \"block53\", \"block54\", \"block55\", \"block56\", \"block57\", \"block58\", \"block59\" \\\n",
    "         \"block60\", \"block61\", \"block62\", \"block63\", \"block64\", \"block65\", \"block66\", \"block67\", \"block68\", \"block69\" \\\n",
    "         \"block70\", \"block71\", \"block72\", \"block73\", \"block74\", \"block75\", \"block76\", \"block77\", \"block78\", \"block79\" \\\n",
    "         \"block80\", \"block81\", \"block82\", \"block83\", \"block84\", \"block85\", \"block86\", \"block87\", \"block88\", \"block89\" \\\n",
    "         \"block90\", \"block91\", \"block92\", \"block93\", \"block94\", \"block95\", \"block96\", \"block97\", \"block98\", \"block99\"]\n",
    "\n",
    "verbs_top = [\"top\", \"above\", \"over\"]\n",
    "verbs_below = [\"below\", \"underneath\", \"next\"]\n",
    "verbs_side = [\"side\", \"neighboring\", \"beside\"]\n",
    "\n",
    "variables_colors = [\"amber\", \"amethyst\", \"apricot\", \"aquamarine\", \"azure\", \"beige\", \"black\", \"blue\", \"blush\", \"bronze\", \"brown\", \"burgundy\", \"byzantium\", \"carmine\", \"cerise\", \"cerulean\", \"champagne\", \"chocolate\", \"coffee\", \"copper\", \"coral\", \"crimson\", \"cyan\", \"emerald\", \"erin\", \"gold\", \"gray\", \"green\", \"harlequin\", \"indigo\", \"ivory\", \"jade\", \"lavender\", \"lemon\", \"lilac\", \"lime\", \"magenta\", \"maroon\", \"mauve\", \"navy\", \"ochre\", \"olive\", \"orange\", \"orchid\", \"peach\", \"pearl\", \"periwinkle\", \"pink\", \"plum\", \"puce\", \"purple\", \"raspberry\", \"red\", \"rose\", \"ruby\", \"salmon\", \"sangria\", \"sapphire\", \"scarlet\", \"silver\", \"tan\", \"taupe\", \"teal\", \"turquoise\", \"ultramarine\", \"violet\", \"viridian\", \"white\", \"yellow\"]\n",
    "variables_location = [\"1\", \"one\", \"2\", \"two\", \"3\", \"three\", \"4\", \"four\", \"5\", \"five\", \"6\", \"six\", \"7\", \"seven\", \"8\", \"eight\", \"9\", \"nine\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Tags:<br/>\n",
    "NN &rightarrow; Noun<br/>\n",
    "VT &rightarrow; Verb Top<br/>\n",
    "VB &rightarrow; Verb Below<br/>\n",
    "VS &rightarrow; Verb Side<br/>\n",
    "VarC &rightarrow; Variable Color<br/>\n",
    "VarL &rightarrow; Variable Location<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Block6 should be on top of block4.', 'Block3 needs to be side by side with block2.', 'Block1 is red.', 'Block5 above Block6.', 'Block3 is at location three, 4, 5.']\n",
      "[('block6', 'NN'), ('top', 'VT'), ('block4', 'NN')]\n",
      "[('block3', 'NN'), ('side', 'VS'), ('side', 'VS'), ('block2', 'NN')]\n",
      "[('block1', 'NN'), ('red', 'VarC')]\n",
      "[('block5', 'NN'), ('above', 'VT'), ('block6', 'NN')]\n",
      "[('block3', 'NN'), ('three', 'VarL'), ('4', 'VarL'), ('5', 'VarL')]\n",
      "\n",
      "Parts of Speech for each sentence\n",
      "[('block6', 'NN'), ('should', 'MD'), ('be', 'VB'), ('on', 'IN'), ('top', 'NN'), ('of', 'IN'), ('block4', 'NN')]\n",
      "[('block3', 'NN'), ('needs', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('side', 'VBN'), ('by', 'IN'), ('side', 'NN'), ('with', 'IN'), ('block2', 'NN')]\n",
      "[('block1', 'NN'), ('is', 'VBZ'), ('red', 'JJ')]\n",
      "[('block5', 'NN'), ('above', 'IN'), ('block6', 'NN')]\n",
      "[('block3', 'NN'), ('is', 'VBZ'), ('at', 'IN'), ('location', 'NN'), ('three', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('5', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "commandFile = open(\"table1.txt\") \n",
    "data = commandFile.read()# Use this to read file content as a stream: \n",
    "\n",
    "sentences = sent_tokenize(data)\n",
    "print(sentences)\n",
    "parts_of_speech = []\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    temp = []\n",
    "    for w in words:\n",
    "        temp.append(w.lower())\n",
    "    words = temp[0:-1]\n",
    "    temp = []\n",
    "    for w in words:\n",
    "        if(w in nouns):\n",
    "            temp.append((w, \"NN\"))\n",
    "        elif(w in verbs_top):\n",
    "            temp.append((w, \"VT\"))\n",
    "        elif(w in verbs_below):\n",
    "            temp.append((w, \"VB\"))\n",
    "        elif(w in verbs_side):\n",
    "            temp.append((w, \"VS\"))\n",
    "        elif(w in variables_colors):\n",
    "            temp.append((w, \"VarC\"))\n",
    "        elif(w in variables_location):\n",
    "            temp.append((w, \"VarL\"))\n",
    "    tagged_words = temp\n",
    "    print(tagged_words)\n",
    "    \n",
    "    #Part of Speech tagging\n",
    "    parts_of_speech.append(nltk.pos_tag(words))\n",
    "    \n",
    "print(\"\\nParts of Speech for each sentence\")\n",
    "for p in parts_of_speech:\n",
    "    print(p)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
